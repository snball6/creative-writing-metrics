---
title: "Creative Writing Data Analysis"
author: "Sarah Ball"
date: "2026-01-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width = "100%")
library(zoo)
install.packages("tidyverse")
library(tidyverse)
library(knitr)

timesheet <- read.csv("data/core_writing_data.csv")
timesheet$date <- as.Date(timesheet$Day, format = "%m/%d/%Y")
# combine entries for the date and category/project.
timesheet <- timesheet %>%
  group_by(date, Day, Time, Category, Project, Sub.Project) %>%
  summarise(Hours = sum(Hours)) %>%
  ungroup()

# combined_workday_schedule is intentionally not included
# in this repo for privacy reasons
schedule <- read.csv("data/combined_workday_schedule.csv")
schedule$date <- as.Date(schedule$date, format = "%Y-%m-%d")

# fill in skip days to create a continuous timeseries
timesheet_complete <- timesheet %>%
  filter(!is.na(date)) %>%
  complete(date = seq(min(date), max(date), by = "day"), fill = list(Hours = 0, Time = "N/A", Category = "N/A", Project = "N/A", Sub.Project = ""))

reduct_c <- "#9ef0f0"
base_color <- "#08bdba"
highlight_c <- "#007d79"
```

With dual degrees in Writing and Computer Science, I have always loved using data to inform my craft. What began as simple word-count tracking of my creative fiction writing, evolved in 2023 into a detailed timesheet that tracked my hours per project—a practice inspired by the timesheets I kept while working as a software developer at Menlo Innovations.

For this project, I applied my analytical and technical skills to my creative writing process to identify insights present in my historical metrics that I could use to inform my writing habits going forward. To do this I enriched my metrics with additional data sources and created visualizations to help contextualize the numbers. I then ran "what-if" scenarios to identify experiments to run and created dashboards for tracking my future progress.

# Quick Links

Looking for examples of how I used specific tools or technologies? Follow these links to jump to the relevant section:

-   [**Spreadsheets**](#spreadsheets) - pivot tables, charts, and formulas

-   [**Python & Data Cleaning**](#python-and-data-cleaning) - python script used to extract, clean, and visualize supplemental schedule data

-   [**R**](#r-programming) - This web page was made using R Markdown and most of the plots were made using the ggplot2 library

-   [**Tableau**](#tracking-dashboard) - I made a dashboard to track my 2026 goals

# Main Data Structure

My creative writing timesheet is tracked in Google Sheets and it includes five fields relevant to this analysis:

-   Day - when the writing occurred. There can be multiple entries for the same day logged against different projects (such as August 10th in the example below)

-   Hours - [active writing time](#active-writing-time "Details on what hours I count") tracked in [15 minute increments](#why-15-minutes "Why 15 minutes") or 0.25 of an hour

-   Category - broad [categorization](#category-explanation "What each category means") about the type of writing activity

-   Project - a shortened title of the story

-   Sub Project - an optional field to group specific efforts on a project

![](images/timesheet_screenshot.png){style="border: 2px solid #595959; padding: 5px; border-radius: 4px; display: block; margin: auto;" width="600"}

Prior to this analysis project, my primary way of visualizing this information and benchmarking my progress was through pivot tables and charts in Google Sheets.

# The Goal {#the-goal}

```{r base timeline, fig.height=4, fig.width=12}
timeline <- read.csv("data/timeline_annotated.csv")
timeline$date <- as.Date(timeline$Day, format = "%m/%d/%Y")
timeline_complete <- timeline %>%
  filter(!is.na(date)) %>%
  complete(date = seq(min(date), max(date), by = "day"), fill = list(Hours = 0, Category = "N/A", Project = "N/A", Sub.Project = "", Timeline.category = ""))

timeline_complete %>%
  # aggregate by month
  mutate(year_month = floor_date(date, "month")) %>%
  group_by(year_month) %>%
  summarise(total_hours = sum(Hours), .groups = "drop") %>%
  # plot
  ggplot(aes(x = year_month, y = total_hours)) +
  geom_col(color = "#ffffff", fill = base_color, show.legend = FALSE) +
  labs(
    title = "Total hours per month",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  scale_y_continuous(expand = expansion(add = 0)) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %Y",
    expand = expansion(mult = c(0, 0))
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

My goal for this project was to identify ways to increase my writing productivity without setting myself an unrealistic or unattainable goal.

Last year, going into 2025, I set myself the goal of writing at least an hour most days—that is, a mode of 1 hour or higher. I thought it was a safe increase to push myself towards given my success in 2024. Surely, my productivity could just keep going up? But in execution, 2025 fell far short of that goal.

```{r 2024 histogram, fig.height=4, fig.width=12}
y_24_25_date_grouped <- timesheet_complete %>%
  filter(year(date) == 2024 | year(date) == 2025) %>%
  group_by(date) %>%
  summarise(sum_hours = sum(Hours))

y_24_25_date_grouped$year_s <- format(y_24_25_date_grouped$date, "%Y")
y_24_25_date_grouped$highlight <- ifelse(
  # mark 2024 mode and median
  (year(y_24_25_date_grouped$date) == 2024 &
    y_24_25_date_grouped$sum_hours == 0.5) |
    # mark 2025 mode and median
    (year(y_24_25_date_grouped$date) == 2025 &
      y_24_25_date_grouped$sum_hours == 0.25),
  "Highlighted", "Normal"
)

median_df <- y_24_25_date_grouped %>%
  group_by(year_s) %>%
  summarise(med_val = median(sum_hours) + (0.25 / 2)) # visually center in bucket
median_df$highlight <- "Normal"

facet_labeller <- as_labeller(c(
  "2024" = "2024 - Mode: 0.50 and Median 1.25",
  "2025" = "2025 - Mode: 0.25 and Median 0.50"
))

ggplot(y_24_25_date_grouped, aes(x = sum_hours, fill = highlight)) +
  geom_histogram(
    binwidth = 0.25,
    boundary = 0,
    closed = "left",
    color = "#FFFFFF",
    show.legend = FALSE
  ) +
  geom_vline(
    data = median_df, aes(xintercept = med_val),
    color = "black", linetype = "dashed", linewidth = 0.5
  ) +
  facet_wrap(~year_s, labeller = facet_labeller) +
  coord_cartesian(xlim = c(0, 8)) +
  scale_x_continuous(expand = expansion(mult = 0, add = 0)) +
  scale_y_continuous(expand = expansion(add = 0)) +
  scale_fill_manual(values = c("Highlighted" = highlight_c, "Normal" = base_color)) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 15),
    panel.spacing = unit(2, "lines")
  ) +
  labs(
    title = "Distribution of writing session length",
    x = "Hours written",
    y = "Number of days"
  )
```

# Behind the Data

To better inform why 2025 fell short of that goal, let me unpack and contextualize my high output year.

```{r annotated timeline, fig.height=4, fig.width=12}
custom_colors <- c(
  "9 none" = "#B0B0B0",
  "5 Sorceress 2nd" = "#444e86",
  "8 1st draft attempt" = "#5886a5",
  "7 Raven 1st" = "#955196",
  "6 Sorceress 3rd" = "#ff6e54",
  "4 Sorceress 4th - R & R" = "#faa600",
  "3 Raven 2nd" = "#dd5182",
  "2 Sorceress 5th" = "#01976C",
  "1 Katarin 1st" = "#97014B",
  "0 Katarin 2nd" = "#015697"
)

timeline_complete %>%
  # aggregate by month
  mutate(year_month = floor_date(date, "month")) %>%
  group_by(year_month, Timeline.category) %>%
  summarise(total_hours = sum(Hours), .groups = "drop") %>%
  # plot
  ggplot(aes(x = year_month, y = total_hours, fill = Timeline.category)) +
  geom_col(color = "#ffffff", show.legend = FALSE) +
  scale_fill_manual(values = custom_colors) +
  labs(
    title = "Total hours per month",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %Y",
    expand = expansion(mult = c(0, 0))
  ) +
  scale_y_continuous(expand = expansion(add = 0)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  # annotate
  annotate("text",
    x = as.Date("2023-02-10"), y = 35,
    label = "Sorceress 2nd",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2023-02-10"), y = 32, xend = as.Date("2023-02-01"), yend = 23,
    curvature = -.01, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2023-05-15"), y = 27,
    label = "1st Draft\nAttempt",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2023-05-15"), y = 20, xend = as.Date("2023-06-15"), yend = 8,
    curvature = 0.3, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2023-09-01"), y = 34,
    label = "Raven 1st",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2023-09-01"), y = 30, xend = as.Date("2023-10-01"), yend = 19,
    curvature = 0.1, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2023-10-30"), y = 53,
    label = "Sorceress 3rd",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2023-11-01"), y = 50, xend = as.Date("2023-12-01"), yend = 38,
    curvature = -0.1, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2024-02-16"), y = 75,
    label = "Sorceress 4th - R&R",
    vjust = "center",
    hjust = "left"
  ) +
  annotate("curve",
    x = as.Date("2024-02-10"), y = 75, xend = as.Date("2024-01-20"), yend = 75,
    curvature = 0, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2024-04-01"), y = 56,
    label = "Raven 2nd",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2024-04-01"), y = 53, xend = as.Date("2024-04-01"), yend = 44,
    curvature = 0, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2024-09-15"), y = 65,
    label = "Sorceress 5th",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2024-09-15"), y = 60, xend = as.Date("2024-08-20"), yend = 45,
    curvature = -.2, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2025-01-01"), y = 58,
    label = "Katarin 1st",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2024-12-25"), y = 55, xend = as.Date("2024-11-18"), yend = 40,
    curvature = -.2, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2025-03-18"), y = 38,
    label = "Pauses",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2025-02-18"), y = 38, xend = as.Date("2025-02-01"), yend = 10,
    curvature = .2, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("curve",
    x = as.Date("2025-04-18"), y = 38, xend = as.Date("2025-05-15"), yend = 18,
    curvature = -.2, arrow = arrow(length = unit(0.3, "cm"))
  ) +
  annotate("text",
    x = as.Date("2025-10-10"), y = 56,
    label = "Katarin 2nd",
    vjust = "center",
    hjust = "center"
  ) +
  annotate("curve",
    x = as.Date("2025-10-10"), y = 52, xend = as.Date("2025-11-01"), yend = 43,
    curvature = 0, arrow = arrow(length = unit(0.3, "cm"))
  )
```

**2023:**

-   [Jan - Sorceress 2nd]{style="background-color:#444e86; color:#FFFFFF; padding:5px; border-radius: 10px"} - I finished the 2nd draft of my Sorceress project and sent out queries to literary agents seeking representation.

-   [Jul - 1st draft attempt]{style="background-color:#5886a5; color:#FFFFFF; padding:5px; border-radius: 10px"} - I started writing a brand new story, but lost steam part way in.

-   [Sep - Raven 1st]{style="background-color:#955196; color:#FFFFFF; padding:5px; border-radius: 10px"} - I decided to participate for the first time in [National Novel Writing Month](https://en.wikipedia.org/wiki/National_Novel_Writing_Month) and used the challenge of 50k words in 30 days to tackle a project I had attempted several times before, but had never finished a full draft. In the months before, I wrote a new outline and the opening chapters. In November, I accomplished the 50k goal, and kept going until the manuscript was finished December 9th for a total of 85k words in 80 days!

-   [Dec - Sorceress 3rd]{style="background-color:#ff6e54; color:#FFFFFF; padding:5px; border-radius: 10px"} - Fresh off that success, I did another rewrite on my Sorceress manuscript and sent out another round of queries.

**2024:**

-   [Jan - Sorceress 4th - R&R]{style="background-color:#faa600; color:#FFFFFF; padding:5px; border-radius: 10px"} - An agent reached out with interest in the story. She gave me some suggested changes and requested I revise and resubmit. I invested all the free time I had in getting the edits in and resubmitted in under two months. Unfortunately, the agent decided not to sign the project.

-   [Mar - Raven 2nd]{style="background-color:#dd5182; color:#FFFFFF; padding:5px; border-radius: 10px"} - Though disappointed about Sorceress, I was eager to dive back into the Raven manuscript and started the 2nd draft.

-   [Jul - Sorceress 5th]{style="background-color:#01976C; color:#FFFFFF; padding:5px; border-radius: 10px"} - I decided to give the Sorceress project one more revision and round of queries. Receiving only form rejections, I decided to shelve the manuscript.

-   [Oct - Katarin 1st]{style="background-color:#97014B; color:#FFFFFF; padding:5px; border-radius: 10px"} - For my 2nd year participating in NaNoWriMo, I worked on a much shorter project and managed the 50k goal in 23 days!

-   [Dec - Raven 2nd]{style="background-color:#dd5182; color:#FFFFFF; padding:5px; border-radius: 10px"} - I was into the thick of the Raven rewrite. Not the fun and exciting opening chapters, but the messy middle where I needed to address plot gaps, character motivations, and point of view.

**2025:**

-   [Feb, May, and Jun - Pauses]{style="background-color:#dd5182; color:#FFFFFF; padding:5px; border-radius: 10px"} - In order to focus on career certification, a job search, and later adjusting to a new job, I put my daily writing habit on pause.

-   [Jul - Raven 2nd]{style="background-color:#dd5182; color:#FFFFFF; padding:5px; border-radius: 10px"} - I resumed my daily writing habit again, but was still working on tricky parts of the manuscript.

-   [Oct - Katarin 2nd]{style="background-color:#015697; color:#FFFFFF; padding:5px; border-radius: 10px"} - Instead of a NaNoWriMo rough draft, I decided to spend November working on Katarin's 2nd draft, and kept going on it through the end of the year.

**So in summary:**

-   In 2023, I was just getting into the swing of writing on a daily basis and gained momentum through NaNoWriMo going into the next year.

-   In 2024, I worked on highly engaging projects like my Sorceress 4th and 5th drafts, the early stages of my Raven 2nd draft, and a quick rough draft.

-   In 2025, I worked almost exclusively on the harder stages of the Raven 2nd draft with several breaks to accommodate life events in the first half of the year. My hours increased some when I began work on the Katarin 2nd draft.

# Building a Model Year

Given the differences, instead of trying to repeat 2024 in 2026, I decided to use my 2024 output as a starting point to create a model year and then apply adjustments to it based on expected differences between 2024 and my anticipated 2026.

## Category Impact

The first high-impact factor was the type of writing I was doing in 2024. It is easier for me to have longer writing sessions when working on 3rd and later drafts. Much of the writing time is spent rereading existing chapters to validate for consistency and continuity. 2nd drafts, by contrast, are high-intensity and I often need frequent breaks. I would compare 3rd+ drafts to a long walk and 2nd drafts to a sprint that is also sometimes uphill.

```{r category histogram 2024, fig.height=3, fig.width=12}
cat_hist <- timesheet_complete

first_median <- median(cat_hist$Hours[cat_hist$Category == "1st Draft"], na.rm = TRUE)
second_median <- median(cat_hist$Hours[cat_hist$Category == "2nd Draft"], na.rm = TRUE)
third_plus_median <- median(cat_hist$Hours[cat_hist$Category == "3rd+ Draft"], na.rm = TRUE)
query_median <- median(cat_hist$Hours[cat_hist$Category == "Querying"], na.rm = TRUE)
free_write_median <- median(cat_hist$Hours[cat_hist$Category == "Free Write"], na.rm = TRUE)

median_df <- cat_hist %>%
  filter(Category %in% c("2nd Draft", "3rd+ Draft")) %>%
  group_by(Category) %>%
  summarise(med_val = median(Hours) + (0.25 / 2)) # visually center in bucket

facet_labeller <- as_labeller(c(
  "1st Draft" = sprintf("1st Draft - Median %.2f", first_median),
  "2nd Draft" = sprintf("2nd Draft - Median %.2f", second_median),
  "3rd+ Draft" = sprintf("3rd+ Draft - Median %.2f", third_plus_median)
))

cat_hist %>%
  filter(Category %in% c("2nd Draft", "3rd+ Draft")) %>%
  ggplot(aes(x = Hours)) +
  geom_histogram(
    binwidth = 0.25,
    boundary = 0,
    closed = "left",
    color = "#FFFFFF",
    fill = base_color,
    show.legend = FALSE,
    position = "identity",
    aes(y = after_stat(density * width)) # probability percent
  ) +
  coord_cartesian(xlim = c(0, 8)) +
  scale_x_continuous(expand = expansion(mult = 0, add = 0)) +
  scale_y_continuous(expand = expansion(add = 0)) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 15),
    panel.spacing = unit(2, "lines")
  ) +
  labs(
    title = "Normalized distribution of writing session length - 2023 through 2025",
    x = "Hours written",
    y = "Percentage of total"
  ) +
  geom_vline(
    data = median_df, aes(xintercept = med_val),
    color = "black", linetype = "dashed", linewidth = 0.5
  ) +
  facet_wrap(~Category, labeller = facet_labeller)
```

Because I do not plan to work on a 3rd or later drafts in 2026, I replaced all my 3rd+ draft writing sessions in the model with my median 2nd draft session length of 0.75.

I also identified three outlier writing sessions in the query category for 2024.

```{r query 2024, fig.height=3, fig.align='center', out.width = "55%"}
q_median <- median(cat_hist$Hours[cat_hist$Category == "Querying"], na.rm = TRUE)

median_df <- cat_hist %>%
  filter(Category %in% c("Querying")) %>%
  group_by(Category) %>%
  summarise(med_val = median(Hours) + (0.25 / 2)) # visually center in bucket

facet_labeller <- as_labeller(c(
  "Querying" = sprintf("Querying - Median %.2f", q_median)
))

cat_hist %>%
  filter(Category %in% c("Querying") & year(date) == 2024) %>%
  ggplot(aes(x = Hours)) +
  geom_histogram(
    binwidth = 0.25,
    boundary = 0,
    closed = "left",
    color = "#FFFFFF",
    fill = base_color,
    show.legend = FALSE,
    position = "identity",
  ) +
  coord_cartesian(xlim = c(0, 8)) +
  scale_x_continuous(expand = expansion(mult = 0, add = 0)) +
  theme_minimal() +
  labs(
    title = "Distribution of writing session length - 2024",
    x = "Hours written",
    y = "Count of days"
  ) +
  geom_vline(
    data = median_df, aes(xintercept = med_val),
    color = "black", linetype = "dashed", linewidth = 0.5
  ) +
  facet_wrap(~Category, labeller = facet_labeller)
```

While I usually spend between a quarter and a half hour on querying every few weeks maintaining a list of potential agents for the projects I am working on, it would be unusual for me to spend any longer sessions on the task in 2026 unless I was working on querying material for a specific project. Since I do not plan to query for a particular manuscript in 2026, I also replaced these writing sessions with my 2nd draft median.

```{r category corrected, fig.height=3, fig.width=12}
model_year <- timesheet_complete %>%
  filter(year(date) == 2024)

# collect totals of incremental adjustments
model_adjustments <- data.frame(
  type = c("2024 Original"),
  yearly_total = c(sum(model_year$Hours, na.rm = TRUE))
)

# 3rd draft adjustment
model_year$cat_adjust_hours <- ifelse(model_year$Category == "3rd+ Draft",
  0.75, model_year$Hours
)

model_adjustments <- rbind(model_adjustments, data.frame(
  type = "3rd+ draft replaced",
  yearly_total = sum(model_year$cat_adjust_hours, na.rm = TRUE)
))

# Query adjustment
model_year$cat_adjust_hours <- ifelse(model_year$Category == "Querying" &
  model_year$Hours > 2,
0.75, model_year$cat_adjust_hours
)

model_adjustments <- rbind(model_adjustments, data.frame(
  type = "Querying replaced",
  yearly_total = sum(model_year$cat_adjust_hours, na.rm = TRUE)
))

# diff for plotting stacked
model_year$cat_adjust_diff <- model_year$Hours - model_year$cat_adjust_hours

model_year %>%
  pivot_longer(
    cols = c(cat_adjust_hours, cat_adjust_diff),
    names_to = "hour_set",
    values_to = "split_hours"
  ) %>%
  # aggregate by month
  mutate(month = floor_date(date, "month")) %>%
  group_by(month, hour_set) %>%
  summarise(
    total_hours = sum(split_hours),
    .groups = "drop"
  ) %>%
  # plot
  ggplot(aes(x = month, y = total_hours, fill = hour_set)) +
  geom_col(color = "#ffffff") +
  scale_fill_manual(
    values = c("cat_adjust_hours" = base_color, "cat_adjust_diff" = reduct_c),
    labels = c("cat_adjust_hours" = "Updated model", "cat_adjust_diff" = "Reduction")
  ) +
  labs(
    title = "Total hours per month",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  theme(legend.title = element_blank()) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %Y",
    expand = expansion(mult = c(0, 0))
  ) +
  scale_y_continuous(expand = expansion(add = 0))
```

## Schedule Changes

Another complicating factor to my dataset was my work schedule. In 2024, I was working a 4 day a week schedule at Menlo Innovations. In April 2025, I switched to a 5 day schedule at Quantum Signal AI. To adjust my model year, I wanted to calculate my median weekday versus weekend writing session length, while also considering outliers in the data.

```{r daily_scatter, fig.height=3, fig.width=12}
# group by day - across categories
model_year_daily <- model_year %>%
  group_by(date = floor_date(date, unit = "day")) %>%
  summarise(
    total_original_hours = sum(Hours),
    cat_adjust_hours = sum(cat_adjust_hours),
    .groups = "drop"
  ) %>%
  mutate(
    day_of_week = weekdays(date),
    # Calculate days to add to reach Sunday - weekday 0
    # If the current day is Sunday (0), add 0 days. If Saturday (6), add 1 day, etc.
    week_ending = date + (7 - as.numeric(format(date, "%w"))) %% 7,
  )


model_year_daily %>%
  ggplot(aes(x = date, y = total_original_hours)) +
  geom_point(color = base_color) +
  labs(
    title = "Daily writing session length - 2024 actuals",
    x = "",
    y = "Hours"
  ) +
  theme_minimal() +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %Y",
    expand = expansion(mult = c(0, 0))
  ) +
  scale_y_continuous(expand = expansion(add = 0))
```

As you can see in the above, my writing hours are highly variable from day-to-day. I knew that the spike up to 8 hours in May 2024 occurred during a vacation week focused almost exclusively on creative writing, but I did not know how many other high or low writing days were on vacations or sick days.

To determine this, I enriched my dataset with information about my work schedule. For more on how I sourced and cleaned my schedule data from my old Menlo timesheets, see [**Python & Data Cleaning**](#python-and-data-cleaning) in the appendix.

This data enrichment allowed me to confirm that paid leave days in general—both sick days and vacation days—included highly variable data. With these days excluded I could calculate my median weekend and weekday writing speed more accurately.

```{r schedule histogram 2024, fig.height=3, fig.width=12}
# Added granularity ultimately no needed - consolidated PTO types
schedule <- schedule %>%
  mutate(schedule_type = case_when(
    schedule_category == "holiday" ~ "paid leave",
    schedule_category == "sickday" ~ "paid leave",
    schedule_category == "vacation" ~ "paid leave",
    schedule_category == "weekend" ~ "weekend",
    schedule_category == "workday" ~ "workday",
  ))

merged <- left_join(model_year_daily, schedule, by = "date")

pto_median <- median(merged$total_original_hours[merged$schedule_type == "paid leave"], na.rm = TRUE)
weekend_median <- median(merged$total_original_hours[merged$schedule_type == "workday"], na.rm = TRUE)
weekday_median <- median(merged$total_original_hours[merged$schedule_type == "weekend"], na.rm = TRUE)

median_df <- merged %>%
  group_by(schedule_type) %>%
  summarise(med_val = median(total_original_hours) + (0.25 / 2)) # visually center in bucket

facet_labeller <- as_labeller(c(
  "paid leave" = sprintf("Paid Leave - Median %.2f", pto_median),
  "workday" = sprintf("Workday - Median %.2f", weekday_median),
  "weekend" = sprintf("Weekend - Median %.2f", weekend_median)
))

merged %>%
  ggplot(aes(x = total_original_hours)) +
  geom_histogram(
    binwidth = 0.25,
    boundary = 0,
    closed = "left",
    color = "#FFFFFF",
    fill = base_color,
    show.legend = FALSE,
    position = "identity",
    aes(y = after_stat(density * width)) # probability percent
  ) +
  geom_vline(
    data = median_df, aes(xintercept = med_val),
    color = "black", linetype = "dashed", linewidth = 0.5
  ) +
  coord_cartesian(xlim = c(0, 8)) +
  scale_x_continuous(expand = expansion(mult = 0, add = 0)) +
  scale_y_continuous(expand = expansion(add = 0)) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 15),
    panel.spacing = unit(2, "lines")
  ) +
  facet_wrap(~schedule_type, labeller = facet_labeller) +
  labs(
    title = "Distribution of writing session length of category adjusted model year",
    x = "Hours written",
    y = "Percentage of total"
  )
```

Because my weekend habit is usually to spend one day heavily focused on chores and life admin tasks, my weekend writing productivity usually varies from day-to-day. So instead of just replacing all Fridays or all Sundays, I selected the highest of the three to replace with my workday median of 1.25 if that day was greater than the median.

```{r schedule corrected, fig.height=3, fig.width=12}
# pivot out for week-wise logic
temp <- model_year_daily %>%
  pivot_wider(
    id_cols = week_ending,
    names_from = day_of_week,
    values_from = cat_adjust_hours
  ) %>%
  mutate(day_to_adjust = case_when(
    Friday > 1.25 &
      Friday >= Saturday &
      Friday >= Sunday ~ "Friday",
    Saturday > 1.25 &
      Saturday >= Friday &
      Saturday >= Sunday ~ "Saturday",
    Sunday > 1.25 &
      Sunday >= Friday &
      Sunday >= Saturday ~ "Sunday",
    TRUE ~ "None"
  ))

# overwrite with adjusted values
temp$Friday <- ifelse(temp$day_to_adjust == "Friday", 1.25, temp$Friday)
temp$Saturday <- ifelse(temp$day_to_adjust == "Saturday", 1.25, temp$Saturday)
temp$Sunday <- ifelse(temp$day_to_adjust == "Sunday", 1.25, temp$Sunday)

# pivot back to tall
temp <- temp %>%
  pivot_longer(
    cols = c(
      "Monday", "Tuesday",
      "Wednesday", "Thursday",
      "Friday", "Saturday", "Sunday"
    ),
    names_to = "day_of_week",
    values_to = "sched_adjusted_hours"
  )

# calculate dates
days_from_sun <- c(
  "Monday" = 6, "Tuesday" = 5, "Wednesday" = 4, "Thursday" = 3,
  "Friday" = 2, "Saturday" = 1, "Sunday" = 0
)
temp$date <- temp$week_ending - days_from_sun[temp$day_of_week]

model_year_daily_merged <- merge(model_year_daily, temp[, c("date", "sched_adjusted_hours")], by = "date")

model_adjustments <- rbind(model_adjustments, data.frame(
  type = "Weekend shortened",
  yearly_total = sum(model_year_daily_merged$sched_adjusted_hours, na.rm = TRUE)
))

# diff for stack plotting
model_year_daily_merged$sched_adjust_diff <- model_year_daily_merged$cat_adjust_hours -
  model_year_daily_merged$sched_adjusted_hours

model_year_daily_merged %>%
  pivot_longer(
    cols = c(sched_adjusted_hours, sched_adjust_diff),
    names_to = "hour_set",
    values_to = "split_hours"
  ) %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(month, hour_set) %>%
  summarise(
    total_hours = sum(split_hours),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = month, y = total_hours, fill = hour_set)) +
  geom_col(color = "#ffffff") +
  scale_fill_manual(
    values = c("sched_adjust_diff" = reduct_c, "sched_adjusted_hours" = base_color),
    labels = c("sched_adjusted_hours" = "Adjusted Hours", "sched_adjust_diff" = "Reduction")
  ) +
  labs(
    title = "Total hours per month",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %Y",
    expand = expansion(mult = c(0, 0))
  ) +
  theme(legend.title = element_blank()) +
  scale_y_continuous(expand = expansion(add = 0))
```

## Model Year Review

```{r model year review, fig.height=3, fig.width=12}
model_year_monthly <- model_year_daily_merged %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(month) %>%
  summarise(
    orig_hours = sum(total_original_hours),
    post_adj_hours = sum(sched_adjusted_hours),
    .groups = "drop"
  )

facet_labeller <- as_labeller(c(
  "orig_hours" = "2024 actuals",
  "post_adj_hours" = "Modified model year"
))

model_year_monthly %>%
  pivot_longer(
    cols = c(orig_hours, post_adj_hours),
    names_to = "hour_set",
    values_to = "split_hours"
  ) %>%
  # plot
  ggplot(aes(x = month, y = split_hours)) +
  geom_col(
    color = "#ffffff",
    fill = base_color
  ) +
  scale_y_continuous(expand = expansion(add = 0)) +
  facet_wrap(~hour_set, labeller = facet_labeller) +
  labs(
    title = "Total hours per month",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b",
    expand = expansion(mult = c(0, 0))
  ) +
  theme(
    panel.spacing = unit(2, "lines")
  )

model_adjustments$weekly_mean <- model_adjustments$yearly_total / 366 * 7 # 2024 was a leap year
```

Above you can see the original and adjusted data side-by-side. While I have primarily displayed my data on a monthly cadence up to this point, for the purposes of planning my goal, I am also interested in a more granular weekly measure, so I calculated the weekly mean based on the yearly total for each adjustment. Broken down individually their impact was:

::: {style="max-width: 400px;"}
|   | **Yearly Total** | **Weekly Mean** |
|-----------------------:|-----------------------:|-----------------------:|
| **2024** | 554.25 | 10.60 |
| **3rd Draft Reduction** | [-102.50]{style="color: red;"} | [-1.96]{style="color: red;"} |
| **Querying Reduction** | [-11.00]{style="color: red;"} | [-0.21]{style="color: red;"} |
| **Schedule Reduction** | [-45.25]{style="color: red;"} | [-0.87]{style="color: red;"} |
| **New Model** | 395.50 | 7.56 |
:::

# What If...?

To identify experiments to try in 2026, I ran "what-if" scenarios against my lower productivity 2025 dataset to see which adjustments might have allowed me to hit my model year output.

```{r 2025 daily baseline, fig.height=3, fig.width=12}
y_25_daily <- timesheet_complete %>%
  filter(year(date) == 2025) %>%
  group_by(date) %>%
  summarise(
    total_orig_hours = sum(Hours),
    .groups = "drop"
  ) %>%
  mutate(
    # Calculate days to add to reach Sunday - weekday 0
    # If the current day is Sunday (0), add 0 days. If Saturday (6), add 1 day, etc.
    week_ending = date + (7 - as.numeric(format(date, "%w"))) %% 7
  )

what_if_adjustments <- data.frame(
  type = c("Starting point"),
  yearly_total = c(sum(y_25_daily$total_orig_hours, na.rm = TRUE))
)
```

## 1) No Skip Days

What if I had maintained my writing streak and written my minimum of 0.25 hours every day. In 2025, I skipped writing 57 days - the bulk of them during the pause periods noted previously.

```{r Skip Days, fig.height=3, fig.width=12}
y_25_daily$skip_adjust_hours <- ifelse(y_25_daily$total_orig_hours == 0,
  0.25, y_25_daily$total_orig_hours
)


what_if_adjustments <- rbind(
  what_if_adjustments,
  data.frame(
    type = "No Skips",
    yearly_total = sum(y_25_daily$skip_adjust_hours, na.rm = TRUE)
  )
)

y_25_daily$skip_adjust_change <- y_25_daily$skip_adjust_hours - y_25_daily$total_orig_hours

y_25_daily %>%
  pivot_longer(
    cols = c(skip_adjust_hours, skip_adjust_change),
    names_to = "hour_set",
    values_to = "split_hours"
  ) %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(month, hour_set) %>%
  summarise(
    total_hours = sum(split_hours),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = month, y = total_hours, fill = hour_set)) +
  geom_col(color = "#ffffff") +
  scale_fill_manual(
    values = c("skip_adjust_change" = highlight_c, "skip_adjust_hours" = base_color),
    labels = c("Increase", "2025 Basline")
  ) +
  labs(
    title = "No Skip Days",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  theme(legend.title = element_blank()) +
  scale_y_continuous(expand = expansion(add = 0)) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b",
    expand = expansion(mult = c(0, 0))
  )
```

## 2) Raised Minimum

What if I had maintained my daily writing streak and also raised my minimum to at least 0.5 hours every day?

```{r Half Hour Min, fig.height=3, fig.width=12}
y_25_daily$min_adjust_hours <- ifelse(y_25_daily$total_orig_hours < 0.5,
  0.5, y_25_daily$total_orig_hours
)

what_if_adjustments <- rbind(
  what_if_adjustments,
  data.frame(
    type = "Raised Minimum",
    yearly_total = sum(y_25_daily$min_adjust_hours, na.rm = TRUE)
  )
)

y_25_daily$min_adjust_change <- y_25_daily$min_adjust_hours - y_25_daily$total_orig_hours

y_25_daily %>%
  pivot_longer(
    cols = c(min_adjust_hours, min_adjust_change),
    names_to = "hour_set",
    values_to = "split_hours"
  ) %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(month, hour_set) %>%
  summarise(
    total_hours = sum(split_hours),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = month, y = total_hours, fill = hour_set)) +
  geom_col(color = "#ffffff") +
  scale_fill_manual(
    values = c("min_adjust_change" = highlight_c, "min_adjust_hours" = base_color),
    labels = c("Increase", "2025 Basline")
  ) +
  labs(
    title = "Raised Minimum",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  theme(legend.title = element_blank()) +
  scale_y_continuous(expand = expansion(add = 0)) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b",
    expand = expansion(mult = c(0, 0))
  )
```

## 3) Two Writing Days

What if I wrote at least 1.5 hours on at least two days a week?

Currently, I often hit this threshold on Wednesdays when I attend a weekly writing meetup. What if I had a second day set aside to extra writing time? To approximate this, I took all weeks that did not already have at least 2 days meeting the criteria, and updated either Wednesday, Saturday, or both days to meet the 2 day requirement.

```{r Twice Weekly, fig.height=3, fig.width=12}
#  note partial weeks at start and end of year already met criteria so no
#  special casing was needed to prevent adding hours for a day in 2024 or 2026

twice <- y_25_daily %>%
  mutate(
    day_of_week = weekdays(date),
    # Calculate days to add to reach Sunday - weekday 0
    # If the current day is Sunday (0), add 0 days. If Saturday (6), add 1 day, etc.
    week_ending = date + (7 - as.numeric(format(date, "%w"))) %% 7
  ) %>%
  pivot_wider(
    id_cols = week_ending,
    names_from = day_of_week,
    values_from = total_orig_hours
  )

threshold <- 1.5

twice$weekly_count <- apply(
  twice[, c(
    "Monday", "Tuesday",
    "Wednesday", "Thursday",
    "Friday", "Saturday", "Sunday"
  )],
  1, function(row) sum(row >= threshold, na.rm = TRUE)
)
twice$adjust <- ifelse(twice$weekly_count < 2, TRUE, FALSE)

# assign Wednesdays
twice$Wednesday <- ifelse(twice$adjust & twice$Wednesday <= threshold, threshold, twice$Wednesday)

# reassess remaining
twice$weekly_count <- apply(
  twice[, c(
    "Monday", "Tuesday",
    "Wednesday", "Thursday",
    "Friday", "Saturday", "Sunday"
  )],
  1, function(row) sum(row >= threshold, na.rm = TRUE)
)
twice$adjust <- ifelse(twice$weekly_count < 2, TRUE, FALSE)
# assign Saturdays
twice$Saturday <- ifelse(twice$adjust & twice$Saturday <= threshold, threshold, twice$Saturday)

# pivot back to tall
twice <- twice %>%
  pivot_longer(
    cols = c(
      "Monday", "Tuesday",
      "Wednesday", "Thursday",
      "Friday", "Saturday", "Sunday"
    ),
    names_to = "day_of_week",
    values_to = "twice_adjusted_hours"
  )

days_from_sun <- c(
  "Monday" = 6, "Tuesday" = 5, "Wednesday" = 4, "Thursday" = 3,
  "Friday" = 2, "Saturday" = 1, "Sunday" = 0
)
twice$date <- twice$week_ending - days_from_sun[twice$day_of_week]

y_25_daily_with_twice <- merge(y_25_daily, twice[, c("date", "twice_adjusted_hours")], by = "date")

what_if_adjustments <- rbind(
  what_if_adjustments,
  data.frame(
    type = "Two Writing Days",
    yearly_total = sum(y_25_daily_with_twice$twice_adjusted_hours, na.rm = TRUE)
  )
)

y_25_daily_with_twice$twice_adjust_change <- y_25_daily_with_twice$twice_adjusted_hours - y_25_daily_with_twice$total_orig_hours


y_25_daily_with_twice %>%
  pivot_longer(
    cols = c(twice_adjusted_hours, twice_adjust_change),
    names_to = "hour_set",
    values_to = "split_hours"
  ) %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(month, hour_set) %>%
  summarise(
    total_hours = sum(split_hours),
    .groups = "drop"
  ) %>%
  ggplot(aes(x = month, y = total_hours, fill = hour_set)) +
  geom_col(color = "#ffffff") +
  scale_fill_manual(
    values = c("twice_adjust_change" = highlight_c, "twice_adjusted_hours" = base_color),
    labels = c("Increase", "2025 Basline")
  ) +
  labs(
    title = "Two Writing Days",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  theme(legend.title = element_blank()) +
  scale_y_continuous(expand = expansion(add = 0)) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b",
    expand = expansion(mult = c(0, 0))
  )
```

## 4) Writing Vacations

What if I took time off from work with the express purpose of spending a large portion of those days writing?

For this one, I did not create a chart, but rather calculated the potential impact.

Given `n` number of writing vacations days and `h` additional hours worked on those days. The total yearly increase would be `h * n`. And the weekly mean change as a result would be `h * n / 365 * 7`

So if I had just one writing-focused vacation day with only one extra hour spent writing the impact would be minimal: +1 hour to the yearly total and +0.02 hours to the weekly mean. At the other end of possibilities, if I took 5 vacations days over the year and wrote an extra 5 hours each of those days I could increase the yearly total by 25 and the weekly mean by 0.48.

## The Results

```{r Calc weekly}
what_if_adjustments$weekly_mean <- what_if_adjustments$yearly_total / 365 * 7
```

::: {style="max-width: 400px;"}
|   | **Yearly Total** | **Weekly Mean** |
|-----------------------:|-----------------------:|-----------------------:|
| **Model** | 395.50 | 7.56 |
| **2025** | 288.25 | 5.53 |
| **No Skips** | [+14.25]{style="color: green;"} | [+0.27]{style="color: green;"} |
| **Raised Minimum** | [+49.25]{style="color: green;"} | [+0.94]{style="color: green;"} |
| **Two Writing Days** | [+53.50]{style="color: green;"} | [+1.03]{style="color: green;"} |
| **Vacation** | [+1 to 25]{style="color: green;"} | [+0.02 to 0.48]{style="color: green;"} |
:::

Looking at the experiments together, the twice weekly option has the highest impact, followed by raising my minimum. If I were to do both I would just fall short at 391 total hours and 7.5 weekly mean. So based on my approximations above, I could feasibly reach my model year goal if I covered the remaining gap with a few vacation writing days, but there would be little room for any contingency.

# Conclusion:

## The Plan:

Given everything I had learned in this analysis, I chose to set my writing goals for the start of 2026 to be:

1)  Write 6 hours total each week

2)  Write at least 1.5 hours two days a week

3)  Write a minimum of 0.5 hours every day

Why only 6 hours? Past experience has taught me to start small and obtainable and increase from there. I decided to round down my weekly model year goal to an even 7.5 hours and set this not as my current weekly goal, but as the weekly total I want to reach by the end of the year. My 2025 weekly mean was only 5.5 hours, so I will start with an increase to 6 hours and try to gradually increase the remaining 1.5 over the course of the year.

I also set no yearly total goal. Weekly goals allow more flexbility and help me stay motivated. If life events disrupt my writing or a productivity experiment is not producing results, I only need to start new the next week, and not be bogged down with the feeling that I need to catch up and work even harder to make up for the shortcoming.

## Tracking Dashboard: {#tracking-dashboard}

To track my progress towards these goals, I created an interactive [Tableau dashboard](https://public.tableau.com/app/profile/sarah.ball5232/viz/WritingMetrics/2026GoalTracking) categorizing my weekly writing time per day alongside the weekly total.

On the left, the colors allow me to see at a glance if I am on track. Ideally I will have at least 2 blue days in a week and the rest will be green. Days that fell short of my 0.5 minimum are colored yellow. And days where  I missed writing entirely appear as red.

![](images/weekly_breakdown_tableau.png){style="border: 2px solid #595959; padding: 5px; border-radius: 4px; display: block; margin: auto;" width="600"}

I also created a dashboard showing my hourly writing data in a heat map calendar format to help me better visualize if there are certain higher or lower output days on which I want to focus.

![](images/heatmap_tableau.png){style="border: 2px solid #595959; padding: 5px; border-radius: 4px; display: block; margin: auto;" width="600"}

On both dashboards, my entire dataset going back to 2023 is available.

Because the public version of Tableau does not allow for live data source linking, the Tableau dashboard will only be updated periodically when I manually reupload and publish my metrics. For a live version, I created a simpler variation of the first dashboard in my [Shared Writing Metrics spreadsheet](https://docs.google.com/spreadsheets/d/1N7T_HbgdGJGohsjXLYBfF8_POvJ1bwDPgBQJ9C8o-Us/edit?gid=1186710364#gid=1186710364). I also added my longest writing streak and calculated my current streak's length.

![](images/sheets_dashboard.png){style="border: 2px solid #595959; padding: 5px; border-radius: 4px; display: block; margin: auto;" width="400"}

As you can see, I fell short of my goals in the the last few weeks, but I do not find that particularly surprising since I had not yet selected my goals, and much of my free time was spent on this analysis. Now that I have selected my goals and have an easy way to track my progress and course correct if needed, I look forward to spending more time on my creative writing in the months ahead!

------------------------------------------------------------------------

# Appendix

## Further Analysis

Prior this year, I did not create separate entries for separate writing sessions that were for the same project on the same day. I might, for convenience, enter the information into my spreadsheet as =1+1, but the same formula might also refer to one 2 hour long writing session where I set a kitchen timer, wrote for an hour, and then decided to keep going.

Because this information was not differentiated, I do not know if multiple short sessions or a few long ones are more common or successful. I also do not know if certain times of day are a better fit. To that end, I added an additional Time field to my tracking sheet that is populated with the start time of my writing session, rounded to the quarter hour. I hope to revisit this data further into the year.

------------------------------------------------------------------------

## Spreadsheets {#spreadsheets}

### Google Sheets vs Microsoft Excel

I have regularly used both Sheets and Excel for work and personal projects, but chose to use Google Sheets for tracking my creative writing time because I already do all my creative writing in Google Docs. I like to do my writing from across multiple devices, including a Chromebook laptop and an Android Phone. This naturally led me towards Google's tools over Microsoft's.

### Shared Writing Metrics

You can view a subset of writing timesheet data [here](https://docs.google.com/spreadsheets/d/1N7T_HbgdGJGohsjXLYBfF8_POvJ1bwDPgBQJ9C8o-Us/edit?usp=sharing) along with the formulas and aggregations I use to track my time. To see which fields are populated through formulas you can use the menu options `View > Show > Formulas` or the shortcut `` ctrl + ` `` to toggle on formula view.

A high level overview of some of the spreadsheet features I am using in this spreadsheet are:

**Formulas:**

-   **Importrange** - I chose to split the data I wanted to publicly share as part of this project from some of the more detailed and project-specific data I track in my timesheet—such as extra notes, word count, pages into a rewrite, and less-polished charts. I still wanted the shared sheet to include live data, rather than a snapshot, so I am importing the core metrics from my live spreadsheet into this shared document.

-   **Countif**, **if, or, today** - On the "Skip Days" tab of the sheet, I am generating a full list of dates starting in 2023 to today and creating fill value rows for days I did not write.

-   **Sort, vstack, filter -** To join the core writing day in with the skip days in the "Skip Days Added" tab, I am using **vstack**. I use **filter** to remove the blank space rows generated by formulas in my core dataset, and I use another **filter** to only stack in those days that are skipped days. Lastly, for easier readability, I **sort** everything in ascending order.

-   **Weekday -** I like to track my writing week from Monday to Sunday so that a single weekend's effort falls in the same week. I use the **weekday** formula and **if** to calculate the Sunday week ending date.

-   **Xlookup -** this is my favorite spreadsheet formula. Introduced to Excel in 2019 and Sheets in 2022, this upgrade to **vlookup** is super powerful and I use it often when aggregating or enriching datasets. In the "2026 Goals" tab, I am using **xlookup** to find the last day where my total hours were 0. From this, I can calculate my current writing streak.

**Pivot Tables:**

-   The tabs "Monthly Totals", "Weekly Totals", and "Daily Writing" each contain pivot tables aggregating the data using date groups at a monthly, weekly, and daily level.

-   For "Monthly Totals" I also used grouped columns to combine my smaller projects to reduce clutter for the chart of monthly writing hours by project.

-   On the "2026 Goals" tab I am using calculated columns to determining the categories for my dashboard chart and filters to only include weeks from 2026.

**Charts:**

-   **Stacked bar** - These plots are my go-to for showing the aggregate total hours worked in a time period broken down by different categories or projects.

-   **Scatter plot** - On the "Daily Totals" tab, I used a scatter plot to show my daily writing because of the density and high variability of the data.

-   **Histograms** - Also on the "Daily Totals" tab, these plots allowed me to review my daily session length by year. More polished versions, created with R for 2024 and 2025 can be seen [in the report above](#the-goal).

-   **Combo Chart** - I do not have an example of this in the shared document, but combo charts are another of my go-to charts. For rewrite projects, I track how many pages in I am on a given day. This allows me to better visualize when I have to double back to earlier in the manuscript. I include hours written on a second axis because I like seeing how the two relate to one another. Sometimes my pages into rewrite barely moves when I am hard at work on a difficult scene, but I can see that I was still putting in a lot of time. Below is a screenshot of what this looks like for the Raven project. In it you can see that in 2024, I frequently had to double back around the 80 page mark to ensure I had the right setup for a particularly difficult scene.

![](images/raven_combo_chart.svg){style="border: 2px solid #595959; padding: 5px; border-radius: 4px; display: block; margin: auto;" width="600"}

------------------------------------------------------------------------

## Active Writing Time {#active-writing-time}

I only count time when I am intentionally sitting down to write, rewrite, or brainstorm on a project.

I do not track time spent thinking about my writing while doing other things. For instance, going out for a walk and mulling over a problem or mentally narrating a scene during my morning commute. I do not count those times because they are harder to quantify. That morning commute narration will start and stop frequently as traffic requires more or less of my focus and my mind will wander to plenty of other things as I walk. The precise number of minutes that were *actually* spent on writing is debatable.

However, I do not want to discount those non-active writing activities. Many of my break through decisions on a plot problem or characterization happen outside of active-writing time. As my dataset grows I hope to revisit the question of non-active writing time and necessary breaks between writing session.

------------------------------------------------------------------------

## Why 15 minutes? {#why-15-minutes}

Gathering metrics comes with a trade-off: the data entry must be easy enough to maintain, yet detailed enough to be meaningful. I chose 15 minutes in part due to my work at Menlo Innovations, where I was already practiced in tracking my time by the quarter-hour.

Interestingly, this increment also serves as a great motivator. If I am five minutes into a new quarter-hour, I am more likely to keep writing to hit the next increment. If I have written for 20 minutes, I might as well push for 30.

------------------------------------------------------------------------

## Category Explanation {#category-explanation}

For my timesheet, I define my categories this way:

-   1st draft - Writing the initial first draft of a story. This is a fast and quick draft with the goal to write out a version of all the scenes in order with minimal editing done as I go.

-   2nd draft - Following the first draft, all the work needed to revise it into complete, coherent, and readable state.

-   3rd+ draft - Includes 3rd and later drafts. Usually just polish or adjustments to further improve the story.

-   Free write - Catch-all category for activities like writing loose scenes for side projects, doing writing exercises, or brainstorming future projects. If a story later progresses to a full 1st draft, some free write entries might get recategorized into the 1st draft category.

-   Querying - Time spent preparing material to query literary agents, researching agents, and any admin or meetings that result.

------------------------------------------------------------------------

## Python & Data Cleaning {#python-and-data-cleaning}

To better understand my writing habits, I wanted to add additional context about my work schedule. Which spikes in writing productivity were due to a vacation days spent writing? Were drops in productivity tied to sick days or a heavy workload?

### Data Source

For my work schedule from April 2025 forward, I could easily source PTO from the application used to submit leave requests. I include memo lines on the request distinguishing between sick days, vacation time, or appointments. Holidays I could pull from an informational email listing the effective holidays for the year.

For data prior April 2025, I would have to get more creative. Outs were submitted through email from an account I no longer have access to, but I do have a nearly complete copy of all my Menlo timesheets from the 2023 to 2025 period of interest. These timesheets do not specify which days were sick days versus planned vacation outs, but they would allow me to assemble a list of days I took PTO and any effective holidays.

### Timesheet Structure

My Menlo timesheets are structure for human readability, not an automated import. Below is an example timesheet with the data I want to pull out marked in blue.

![](images/annotated_menlo_timesheet.png){style="border: 2px solid #595959; padding: 5px; border-radius: 4px; display: block; margin: auto;" width="600"}

Given the complexity of file format and the number of files (118 in total), I chose to write a python script to extract the information I needed into an easier csv format. My script needed to dynamically handle the varying number of rows and slight changes that occured to the template layout over the years. My entire python script can be found [here](https://github.com/snball6/creative-writing-metrics/blob/main/python-timesheet-parsing/timesheet_parser.py), but the parsing specifically occurs in the `parse_timesheets` function.

For the example above, my script would mark Saturday, Sunday, and Friday as weekends, Monday and Tuesday as workdays, Wednesday as PTO, and Thursday as a Holiday.

### Cleaning

After I initially parsed the data, I created a calendar heatmap visualization to help me validate the output. It quickly revealed some potential issues in the data. For example, it looked like I had taken three full weeks of vacation in April! And I saw blank weeks in other years that did not line up with any vacations I remembered taking.

![](images/work_schedule_heatmap.png){style="border: 2px solid #595959; padding: 5px; border-radius: 4px; display: block; margin: auto;" width="600"}

I added an additional check to my script to not only list any files that failed to parse, but to also check for any missing dates in the data. The results gave me a clear list of files to manually check, and an answer to why the missing weeks were not in my failed to parse list. The timesheets for those weeks had the right date in their file name, but the wrong one in their cells, resulting in those date's values being overwritten.

Because I wanted to be able to reproduce my work, I kept a running list of all my changes to the files–checked into the same repo as my parsing script and this report. I was also working on a duplicate copy of my timesheet files, so I would always have the originals available for reference or to revert my changes.

There was one file I knew was missing before I even started my analysis. I decided to make a placeholder best-guess timesheet to bridge that gap. Because my goal for using this data was to identify non-work days that might explain outlier hours and I had already verified there were no outliers, I decided it was sufficient to assume a regular work week schedule. 

If there had been outliers in that week or I had more missing weeks in the dataset, I would have introduced an "unknown" category to help distinguish these days.

### Manual-Categorizing:

Once the script was able to produce a set of PTO days, I began working through them one at a time to see if I could further classify them. By checking my calendars, emails, and my journals, I was able to add the additional granularity for sick days and vacation days.

I created an override file listing the date, category, and source/reason for my grouping for my own reference and then added into the script a step to use these overrides to add the additional categories.

Ultimately, I decided not to include the extra granularity in my overall analysis. Both sick days and vacation days had highly variable writing session lengths. Not all vacation days were writing focused. Busy holiday gatherings might result in less writing. While sick days were often skip or only quarter an hour writing sessions, there were still instances of high output writing days. In cases where I stayed home sick due to a migraine, but felt well enough by afternoon it would not be unusual for me to be restless to be doing something and choose to invest that time in an hour or more of writing.

```{r PTO categories, fig.height=3, fig.width=12}
merged_complete <- timesheet_complete %>%
  group_by(date = floor_date(date, unit = "day")) %>%
  summarise(
    total_hours = sum(Hours)
  ) %>%
  left_join(schedule, by = "date") %>%
  filter(schedule_category %in% c("holiday", "sickday", "vacation"))

merged_complete %>%
  ggplot(aes(x = total_hours)) +
  geom_histogram(
    binwidth = 0.25,
    boundary = 0,
    closed = "left",
    color = "#FFFFFF",
    fill = base_color,
    show.legend = FALSE,
    position = "identity",
    aes(y = after_stat(density * width)) # probability percent
  ) +
  coord_cartesian(xlim = c(0, 8)) +
  scale_x_continuous(expand = expansion(mult = 0, add = 0)) +
  scale_y_continuous(expand = expansion(add = 0)) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 15),
    panel.spacing = unit(2, "lines")
  ) +
  facet_wrap(~schedule_category) +
  labs(
    title = "Normalized distribution of writing session length",
    x = "Hours written",
    y = "Percentage of total"
  )
```

------------------------------------------------------------------------

## R Programming {#r-programming}

This report page was written in R Markdown and the html page generated using the knitr library. The .rmd version can be seen [here](https://github.com/snball6/creative-writing-metrics/blob/main/docs/index.Rmd) including the code chunks used to aggregate and alter my dataset and generate the plots of this report.

------------------------------------------------------------------------

## What do you write? Are you published?

I write primarily fiction, usually in the adult fantasy or science fiction genres. I am not published yet, but ultimately finding an agent and a traditional publisher for my books is my ultimate goal!

------------------------------------------------------------------------

## How long does it take you to write a novel?

That is the very question that started me tracking my time in the first place. At present, I do not have hourly data for a manuscript from beginning to end. For my most-finished work, the Sorceress project, I only have the home stretch of the 2nd draft tracked, followed by the 3rd, 4th, and 5th drafts.

Based on the data I do have, I can say that I write rough drafts pretty quickly. Raven took me 50.25 hours and Katarin took me 31.5.

2nd drafts take me much longer. Raven is at 500 hours and counting.

Trying to approximate a total based on the data I do have available is something I hope to explore in a future analysis.
